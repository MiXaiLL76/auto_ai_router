# Production configuration example
# High-availability setup with monitoring and strict rate limits

server:
  port: 8080
  master_key: "sk-production-master-key-change-me"
  logging_level: info
  replace_v1_models: true
  request_timeout: 10m
  default_models_rpm: 100
  max_body_size_mb: 20

credentials:
  # Primary providers with rate limits
  - name: "openai_tier1"
    api_key: "sk-proj-tier1-key"
    base_url: "https://api.openai.com"
    rpm: 100

  - name: "openai_tier2"
    api_key: "sk-proj-tier2-key"
    base_url: "https://api.openai.com"
    rpm: 100

  - name: "openai_tier3"
    api_key: "sk-proj-tier3-key"
    base_url: "https://api.openai.com"
    rpm: 100

  # Fallback providers
  - name: "azure_fallback"
    api_key: "azure-key"
    base_url: "https://your-resource.openai.azure.com"
    rpm: 200

fail2ban:
  max_attempts: 5
  ban_duration: permanent
  error_codes: [401, 403, 429, 500, 502, 503, 504]

# Note: models.yaml will be auto-generated with per-model rate limits
# You can manually edit it to set custom RPM for specific models
