server:
  port: 8080
  max_body_size_mb: 100  # Maximum request body size in MB (default: 100)
  response_body_multiplier: 10  # Response body limit = max_body_size_mb * this value (default: 10)
  request_timeout: 60s  # Request timeout (default: 60s)
  write_timeout: 60s  # HTTP server write timeout (default: 60s)
  idle_timeout: 2m  # HTTP server idle timeout (default: 2*write_timeout)
  idle_conn_timeout: 120s  # HTTP idle connection timeout (default: 120s)
  max_idle_conns: 200  # Maximum idle connections (default: 200)
  max_idle_conns_per_host: 20  # Maximum idle connections per host (default: 20)
  logging_level: info  # Options: info, debug, error (default: info)
  master_key: "sk-your-master-key-here"  # Required: Master key for authentication
  default_models_rpm: -1  # Default RPM limit for models (-1 for unlimited, default: -1)
  model_prices_link: ""  # Optional: URL or file path to model prices JSON (supports os.environ/VAR_NAME)

fail2ban:
  max_attempts: 3
  ban_duration: permanent  # Use 'permanent' for permanent bans, or duration like '1h30m'
  error_codes: [401, 403, 429, 500, 502, 503, 504]
  # Optional: per-error-code rules override global max_attempts
  # error_code_rules:
  #   - code: 429
  #     max_attempts: 5
  #     ban_duration: 5m

monitoring:
  prometheus_enabled: true
  log_errors: false
  errors_log_path: "logs/logs.jsonl"

credentials:
  # Direct provider credentials
  - name: "openai_main"
    type: "openai"
    api_key: "sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    base_url: "https://api.openai.com"
    rpm: 100
    tpm: 50000

  - name: "vertex_ai"
    type: "vertex-ai"
    project_id: "your-project-id"
    location: "global"
    credentials_file: "path/to/service-account.json"
    rpm: 100
    tpm: 50000

  # Proxy credential (forwards requests to another auto_ai_router instance)
  - name: "proxy_fallback"
    type: "proxy"
    base_url: "http://backup-router.local:8080"  # URL of remote auto_ai_router
    api_key: "sk-remote-master-key"  # Optional: remote master key
    rpm: 200
    tpm: 100000
    is_fallback: true  # Use as fallback when primary credentials are exhausted

# Optional: Models with specific credential binding
models:
  - name: "gpt-4o"
    credential: openai_main
    rpm: 100
    tpm: 50000
  - name: "gemini-2.5-pro"
    credential: vertex_ai
    rpm: 100
    tpm: 50000

# Optional: LiteLLM database integration for spend logging and auth
litellm_db:
  enabled: false  # Set to true to enable LiteLLM DB integration
  is_required: false  # Set to true to fail startup if DB connection fails
  database_url: "os.environ/LITELLM_DATABASE_URL"  # PostgreSQL connection string (supports env variables)
  max_conns: 25  # Maximum database connections (default: 25, optimized for ~1000 RPM)
  min_conns: 5  # Minimum database connections (default: 5)
  health_check_interval: 10s  # Health check interval (default: 10s)
  connect_timeout: 5s  # Connection timeout (default: 5s)
  auth_cache_ttl: 20s  # Auth cache TTL (default: 20s)
  auth_cache_size: 10000  # Auth cache size (default: 10000)
  log_queue_size: 5000  # Spend log queue size (default: 5000)
  log_batch_size: 100  # Spend log batch size (default: 100)
  log_flush_interval: 5s  # Spend log flush interval (default: 5s)
  log_retry_attempts: 3  # Spend log retry attempts (default: 3)
  log_retry_delay: 1s  # Spend log retry delay (default: 1s)
