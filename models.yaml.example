# Model-level RPM and TPM configuration
# This file is auto-generated when the router starts and fetches models from credentials
# You can manually edit RPM/TPM values for specific models

models:
  - name: gpt-4o
    rpm: 50     # Requests per minute limit for this model (per credential)
    tpm: 40000  # Tokens per minute limit for this model (per credential)

  - name: gpt-4o-mini
    rpm: 100    # Higher limit for cheaper model
    tpm: 80000

  - name: gpt-3.5-turbo
    rpm: 150    # Even higher for fastest model
    tpm: -1     # Unlimited tokens

  - name: text-embedding-3-small
    rpm: 200    # High limit for embeddings
    tpm: 0      # No token limit (use 0 or -1)

# Notes:
# - If a model is not listed here, default_models_rpm from config.yaml will be used
# - Model limits are applied to EACH credential independently (combined approach)
# - Effective limit = MIN(credential_limit, model_limit)
# - Example: If credential has rpm=100 and model has rpm=60, effective limit is 60
# - New models discovered during startup will be automatically added with default_models_rpm and tpm=-1
# - Use -1 for rpm or 0/-1 for tpm to disable that limit
# - You can edit this file and restart the router to apply changes
